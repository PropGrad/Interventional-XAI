import numpy as np
from PIL import Image
import torch as T
import diffusers
import warnings
from tqdm import tqdm
from pathlib import Path


def pix2pix_edit(img, instruction, cfg_txt=np.linspace(1.01, 12.5, 100, endpoint=True), cfg_img=2.5, SEED=42, verbose=False):
    """Edit an image using the Stable Diffusion Instruct Pix2Pix pipeline and generate outputs
    for a text instruction and increasing image guidance scales.

    Parameters
    ----------
    img : PIL.Image.Image or ndarray or str or Path
        The input image to be edited. Can be a PIL Image, a NumPy array convertible to an image,
        or a file path to an image.
    instruction : str
        Natural-language instruction describing the desired edit to apply to the input image.
    cfg_txt : iterable of float, optional
        Iterable of guidance_scale values (text guidance strength) to sweep over when generating outputs.
        Each value in this iterable is passed as the pipeline's guidance_scale. Default:
        np.linspace(1.01, 12.5, 100, endpoint=True).
    cfg_img : float or iterable of float, optional
        Image guidance strength(s) (image_guidance_scale) that control how strongly the input image is preserved.
        The function iterates over this argument; therefore pass a sequence (e.g. [2.5]) if you want a single
        value. Defaults to 2.5 in the signature, but the implementation expects an iterable.
    SEED : int, optional
        Random seed used to initialize the generator for deterministic outputs. Defaults to 42.
    verbose : bool, optional
        If True, prints a short progress message showing the instruction being used. Defaults to False.

    Returns
    -------
    list of PIL.Image.Image
        A list of edited images generated by the pipeline for each guidance_scale value in cfg_txt.
    """
    warnings.simplefilter("ignore")
    device = 'cuda' if T.cuda.is_available() else 'cpu'
    pipe = diffusers.StableDiffusionInstructPix2PixPipeline.from_pretrained(
        'timbrooks/instruct-pix2pix', torch_dtype=T.float16, safety_checker=None).to(device)
    pipe.set_progress_bar_config(disable=True)

    if isinstance(img, str) or isinstance(img, Path):
        img = Image.open(img).convert('RGB')
    elif isinstance(img, np.ndarray):
        img = Image.fromarray(img.astype('uint8')).convert('RGB')

    interventional_data = []
    with T.inference_mode():
        if verbose:
            print(f"Pix2Pix Instruction: {instruction}")
            
        for cfg_t in tqdm(cfg_txt, disable=not verbose):
            res = pipe(prompt=instruction, image=img, generator=T.Generator(device=device).manual_seed(SEED), guidance_scale=cfg_t,
                        image_guidance_scale=cfg_img).images[0]
            interventional_data.append(res)

    return interventional_data
